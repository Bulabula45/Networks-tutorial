{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a0eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdf41505",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 1, 2\n",
    "kernel_size = 3\n",
    "batch_size = 2\n",
    "bias = False\n",
    "input_size = [batch_size, in_channels, 4, 4]\n",
    "conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57bd3654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0322, -0.1890],\n",
      "          [-0.0992,  0.4416]],\n",
      "\n",
      "         [[ 0.1671, -0.4136],\n",
      "          [ 0.1992,  0.1659]]],\n",
      "\n",
      "\n",
      "        [[[-0.2014, -0.3555],\n",
      "          [-1.0872, -0.1500]],\n",
      "\n",
      "         [[ 0.8817, -0.2720],\n",
      "          [-1.3173, -0.2607]]]], grad_fn=<ConvolutionBackward0>)\n",
      "------------------------------------------------------------------------------------------\n",
      "tensor([[[[ 0.0322, -0.1890],\n",
      "          [-0.0992,  0.4416]],\n",
      "\n",
      "         [[ 0.1671, -0.4136],\n",
      "          [ 0.1992,  0.1659]]],\n",
      "\n",
      "\n",
      "        [[[-0.2014, -0.3555],\n",
      "          [-1.0872, -0.1500]],\n",
      "\n",
      "         [[ 0.8817, -0.2720],\n",
      "          [-1.3173, -0.2607]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_feature_map = torch.randn(input_size)\n",
    "output_feature_map = conv_layer(input_feature_map)\n",
    "print(output_feature_map)\n",
    "print(\"---\"*30)\n",
    "output_feature_map1 = F.conv2d(input_feature_map, conv_layer.weight)\n",
    "print(output_feature_map1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "725f4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = input_feature_map\n",
    "kernel = conv_layer.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3782f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 4, 4]), torch.Size([2, 1, 3, 3]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "310ee0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "#step1 原始的矩阵运算实现二维卷积\n",
    "def matrix_multiplication_for_conv2d(inputs, kernel, stride=1, bias=None, padding=0):\n",
    "    if padding > 0:\n",
    "        inputs = F.pad(inputs, (padding, padding, padding, padding))\n",
    "    \n",
    "    bs, in_channels, input_h, input_w = inputs.shape\n",
    "    out_channels, _, kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    \n",
    "    output_w = floor((input_w - kernel_w) / stride) + 1  #上取整\n",
    "    output_h = floor((input_h - kernel_h) / stride) + 1\n",
    "    \n",
    "    output = torch.zeros((bs, out_channels, output_h, output_w))\n",
    "    \n",
    "    for i in range(0, input_h-kernel_h+1, stride):\n",
    "        for j in range(0, input_w-kernel_w+1, stride):\n",
    "            for c_out in range(out_channels):\n",
    "                for c_in in range(in_channels):\n",
    "                    output[:, c_out, i//stride, j//stride] += torch.sum(torch.mul(inputs[:, c_in, i:i+kernel_h, j:j+kernel_w], kernel[c_out, c_in, :, :]), \\\n",
    "                                                       dim=(1, 2))\n",
    "                if bias is not None:\n",
    "                    output[:, c_out, i//stride, j//stride] += bias[c_out]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d908256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.3310,  0.0360, -0.2374, -0.1558],\n",
      "          [ 0.2562,  0.0322, -0.1890, -0.1495],\n",
      "          [ 0.0475, -0.0992,  0.4416,  0.3913],\n",
      "          [-0.3610,  0.2912,  0.4568,  0.2339]],\n",
      "\n",
      "         [[-0.0803, -0.5396, -0.2961, -0.1268],\n",
      "          [ 0.0072,  0.1671, -0.4136, -0.5437],\n",
      "          [-0.1491,  0.1992,  0.1659,  0.2691],\n",
      "          [-0.0036, -0.1429, -0.3807,  0.1655]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6435, -0.1779, -0.1162, -0.3956],\n",
      "          [-0.4974, -0.2014, -0.3555, -0.5743],\n",
      "          [-0.2876, -1.0872, -0.1500,  0.3378],\n",
      "          [-0.0538, -0.0606,  0.0077,  0.8676]],\n",
      "\n",
      "         [[ 0.7068,  0.1829, -0.1004, -0.0776],\n",
      "          [-0.1209,  0.8817, -0.2720, -0.5528],\n",
      "          [-0.1386, -1.3173, -0.2607, -0.1748],\n",
      "          [ 0.5444,  0.3498, -0.4395,  0.1488]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3310,  0.0360, -0.2374, -0.1558],\n",
       "          [ 0.2562,  0.0322, -0.1890, -0.1495],\n",
       "          [ 0.0475, -0.0992,  0.4416,  0.3913],\n",
       "          [-0.3610,  0.2912,  0.4568,  0.2339]],\n",
       "\n",
       "         [[-0.0803, -0.5396, -0.2961, -0.1268],\n",
       "          [ 0.0072,  0.1671, -0.4136, -0.5437],\n",
       "          [-0.1491,  0.1992,  0.1659,  0.2691],\n",
       "          [-0.0036, -0.1429, -0.3807,  0.1655]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6435, -0.1779, -0.1162, -0.3956],\n",
       "          [-0.4974, -0.2014, -0.3555, -0.5743],\n",
       "          [-0.2876, -1.0872, -0.1500,  0.3378],\n",
       "          [-0.0538, -0.0606,  0.0077,  0.8676]],\n",
       "\n",
       "         [[ 0.7068,  0.1829, -0.1004, -0.0776],\n",
       "          [-0.1209,  0.8817, -0.2720, -0.5528],\n",
       "          [-0.1386, -1.3173, -0.2607, -0.1748],\n",
       "          [ 0.5444,  0.3498, -0.4395,  0.1488]]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_feature_map1 = F.conv2d(input_feature_map, conv_layer.weight, padding=1)\n",
    "print(output_feature_map1)\n",
    "print(\"---\"*30)\n",
    "matrix_multiplication_for_conv2d(inputs, kernel, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3339e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0435,  0.0365],\n",
      "          [-0.2333, -0.1044]],\n",
      "\n",
      "         [[-0.1303,  0.4713],\n",
      "          [ 0.2422,  0.2818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3067,  0.1343],\n",
      "          [-0.2522,  0.5407]],\n",
      "\n",
      "         [[ 0.3796,  0.5556],\n",
      "          [ 0.8235,  0.5901]]]], grad_fn=<ConvolutionBackward0>)\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0435,  0.0365],\n",
       "          [-0.2333, -0.1044]],\n",
       "\n",
       "         [[-0.1303,  0.4713],\n",
       "          [ 0.2422,  0.2818]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3067,  0.1343],\n",
       "          [-0.2522,  0.5407]],\n",
       "\n",
       "         [[ 0.3796,  0.5556],\n",
       "          [ 0.8235,  0.5901]]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, bias=True)\n",
    "bias_output_feature_map = bias_conv_layer(input_feature_map)\n",
    "print(bias_output_feature_map)\n",
    "print(\"---\"*30)\n",
    "matrix_multiplication_for_conv2d(inputs, bias_conv_layer.weight.data, bias=bias_conv_layer.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29d555dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2 向量内积实现\n",
    "def flatten_multiplication_for_conv2d(inputs, kernel,stride=1,bias=0,padding=0):\n",
    "    input_h, input_w = inputs.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_w = floor((input_w - kernel_w) / stride) + 1\n",
    "    output_h = floor((input_h - kernel_h) / stride) + 1\n",
    "    output = torch.zeros(output_h, output_w)\n",
    "    \n",
    "    region_matrix = torch.zeros(output.numel(), kernel.numel())  #存储输入拉平后\n",
    "    kernel_matrix = torch.reshape(kernel, (-1,1))  #kernel转成列向量，其实是矩阵\n",
    "    row_index = 0\n",
    "    for i in range(0, input_h-kernel_h+1, stride):\n",
    "        for j in range(0, input_w-kernel_w+1, stride):\n",
    "            region = inputs[i:i+kernel_h, j:j+kernel_w]\n",
    "            region_vector = torch.flatten(region) #取出被核滑动的输入区域，转成行向量\n",
    "            region_matrix[i//stride * output_w + j//stride,:] = region_vector\n",
    "            #region_matrix[row_index] = region_vactor\n",
    "            #row_index += 1\n",
    "    output_matrix = region_matrix @ kernel_matrix\n",
    "    output = output_matrix.reshape(output_h, output_w)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "164667e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0903, -1.2699],\n",
      "        [ 2.1137,  0.9855]])\n",
      "------------------------------------------------------------------------------------------\n",
      "tensor([[[[ 3.0903, -1.2699],\n",
      "          [ 2.1137,  0.9855]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "output_feature_map = flatten_multiplication_for_conv2d(inputs, kernel, stride=2)\n",
    "print(output_feature_map)\n",
    "print(\"---\"*30)\n",
    "output_feature_map1 = F.conv2d(inputs.reshape((1,1,inputs.shape[0], inputs.shape[1])), kernel.reshape((1,1,kernel.shape[0], kernel.shape[1])), stride=2)\n",
    "print(output_feature_map1)\n",
    "torch.allclose(output_feature_map, output_feature_map1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a246e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5, 5]) torch.Size([4, 2, 3, 3]) torch.Size([2, 3, 3, 3]) torch.Size([2])\n",
      "tensor([[[[-0.1456,  0.8013, -1.0964],\n",
      "          [-0.0609, -0.0595, -0.2931],\n",
      "          [-1.0771,  0.2870, -0.9871]],\n",
      "\n",
      "         [[ 0.1348, -0.8107,  0.1260],\n",
      "          [-0.5127, -0.6665, -0.2516],\n",
      "          [-1.1275, -0.0873,  0.9869]]],\n",
      "\n",
      "\n",
      "        [[[-0.4831,  0.7683,  0.1324],\n",
      "          [ 0.8327,  0.2712, -0.3787],\n",
      "          [-0.0966, -0.2022, -0.6484]],\n",
      "\n",
      "         [[-0.0431, -0.4704,  0.3328],\n",
      "          [-0.1602,  0.2798,  0.0251],\n",
      "          [ 0.1935,  0.0363, -0.5138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2664,  0.5296,  0.2496],\n",
      "          [ 1.3215,  0.8688,  0.8170],\n",
      "          [ 0.3469,  0.4899,  0.5967]],\n",
      "\n",
      "         [[ 0.0561, -0.3504, -0.1132],\n",
      "          [-0.0435, -0.6647,  0.0925],\n",
      "          [-0.0901,  0.3814, -0.1853]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4767,  0.6284,  0.0255],\n",
      "          [-0.0665, -0.3060, -0.7124],\n",
      "          [-0.0923,  0.1403,  0.5062]],\n",
      "\n",
      "         [[-0.0350,  0.7030,  0.3673],\n",
      "          [ 0.0914, -0.4134, -0.3617],\n",
      "          [ 0.4065,  0.5488, -0.4041]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in_channels, out_channels, kernel_size = 3, 2, 3\n",
    "batch_size = 4\n",
    "input_size = [batch_size, in_channels, 5, 5]\n",
    "bias = True\n",
    "conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias)\n",
    "inputs = torch.randn(input_size)\n",
    "output = conv_layer(inputs)\n",
    "kernel = conv_layer.weight.data\n",
    "bias = conv_layer.bias.data\n",
    "print(inputs.shape, output.shape, kernel.shape, bias.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "94548421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 9, 9])\n",
      "torch.Size([2, 3, 9, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1456,  0.8013, -1.0964],\n",
       "          [-0.0609, -0.0595, -0.2931],\n",
       "          [-1.0771,  0.2870, -0.9871]],\n",
       "\n",
       "         [[ 0.1348, -0.8107,  0.1260],\n",
       "          [-0.5127, -0.6665, -0.2516],\n",
       "          [-1.1275, -0.0873,  0.9869]]],\n",
       "\n",
       "\n",
       "        [[[-0.4831,  0.7683,  0.1324],\n",
       "          [ 0.8327,  0.2712, -0.3787],\n",
       "          [-0.0966, -0.2022, -0.6484]],\n",
       "\n",
       "         [[-0.0431, -0.4704,  0.3328],\n",
       "          [-0.1602,  0.2798,  0.0251],\n",
       "          [ 0.1935,  0.0363, -0.5138]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2664,  0.5296,  0.2496],\n",
       "          [ 1.3215,  0.8688,  0.8170],\n",
       "          [ 0.3469,  0.4899,  0.5967]],\n",
       "\n",
       "         [[ 0.0561, -0.3504, -0.1132],\n",
       "          [-0.0435, -0.6647,  0.0925],\n",
       "          [-0.0901,  0.3814, -0.1853]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4767,  0.6284,  0.0255],\n",
       "          [-0.0665, -0.3060, -0.7124],\n",
       "          [-0.0923,  0.1403,  0.5062]],\n",
       "\n",
       "         [[-0.0350,  0.7030,  0.3673],\n",
       "          [ 0.0914, -0.4134, -0.3617],\n",
       "          [ 0.4065,  0.5488, -0.4041]]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step3 考虑bs、channels的向量内积实现\n",
    "def complete_flatten_multiplication_for_conv2d(inputs, kernel, stride=1, bias=None, padding=0):\n",
    "    if padding > 0:\n",
    "        inputs = F.pad(inputs, (padding, padding, padding, padding, 0, 0, 0, 0)) #f.pad函数先height,width后bs,channel\n",
    "    \n",
    "    bs, in_channels, input_h, input_w = inputs.shape\n",
    "    out_channels, _, kernel_h, kernel_w = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channels)\n",
    "    \n",
    "    output_w = floor((input_w - kernel_w) / stride) + 1  #上取整\n",
    "    output_h = floor((input_h - kernel_h) / stride) + 1\n",
    "    output = torch.zeros((bs, out_channels, output_h, output_w))\n",
    "    \n",
    "    region_matrix = torch.zeros((bs,in_channels,output_h*output_w, kernel_h*kernel_w))  #存储输入拉平后\n",
    "    kernel_matrix = torch.reshape(kernel, (out_channels, in_channels,kernel_h*kernel_w,1))  #kernel转成列向量，其实是矩阵\n",
    "    out_matrix = torch.zeros((bs, out_channels, output_h*output_w, 1))\n",
    "  \n",
    "    for c_in in range(in_channels):\n",
    "        row_index = 0\n",
    "        for i in range(0, input_h-kernel_h+1, stride):\n",
    "            for j in range(0, input_w-kernel_w+1, stride):\n",
    "                region = inputs[:, c_in, i:i+kernel_h, j:j+kernel_w].reshape((bs, -1))\n",
    "                region_matrix[:, c_in, row_index, :] = region\n",
    "                row_index += 1\n",
    "    print(region_matrix.shape)\n",
    "    print(kernel_matrix.shape)\n",
    "    for c_out in range(out_channels):       \n",
    "        for c_in in range(in_channels):\n",
    "            out_matrix[:, c_out, :, :] += torch.bmm(region_matrix[:, c_in], kernel_matrix[c_out, c_in].unsqueeze(0).tile(bs, 1, 1))\n",
    "        out_matrix[:, c_out, :, :] += bias[c_out]\n",
    "    output = out_matrix.reshape((bs, out_channels, output_h, output_w))\n",
    "    \n",
    "    return output\n",
    "\n",
    "complete_flatten_multiplication_for_conv2d(inputs, kernel, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba5d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
