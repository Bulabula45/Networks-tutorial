{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718e97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08c7f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2335,  0.0344, -0.2780,  0.5216,  0.0529],\n",
      "         [-0.2386,  0.3331, -0.3291,  0.3712, -0.3144],\n",
      "         [-0.1060,  0.0798, -0.2508,  0.0960, -0.2757]],\n",
      "\n",
      "        [[ 0.0434,  0.0873, -0.0879,  0.1100,  0.5907],\n",
      "         [-0.0907,  0.0738,  0.1541,  0.0971,  0.2843],\n",
      "         [-0.2087,  0.2055,  0.2546,  0.2179, -0.1388]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.1060,  0.0798, -0.2508,  0.0960, -0.2757],\n",
      "         [-0.2087,  0.2055,  0.2546,  0.2179, -0.1388]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.2106,  0.1742, -0.4524,  0.1758, -0.5244],\n",
      "         [-0.5820,  0.2689,  0.3088,  0.3347, -0.1883]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "proj_size = 3\n",
    "inputs = torch.randn(bs, T, i_size)\n",
    "h_0 = torch.randn(bs, h_size)\n",
    "c_0 = torch.randn(bs, h_size)\n",
    "#h_0 = torch.randn(bs, proj_size) #proj是对h压缩输出\n",
    "\n",
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first=True)\n",
    "output, (hn, cn) = lstm_layer(inputs, (h_0.unsqueeze(0), c_0.unsqueeze(0)))\n",
    "\n",
    "print(output)\n",
    "print(hn)\n",
    "print(cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcfe80f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([1, 2, 5])\n",
      "torch.Size([1, 2, 5])\n",
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(hn.shape)\n",
    "print(cn.shape)\n",
    "for k,v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape) #4*5,4  4*5,5 四个矩阵拼接在一起的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7136b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2335,  0.0344, -0.2780,  0.5216,  0.0529],\n",
      "         [-0.2386,  0.3331, -0.3291,  0.3712, -0.3144],\n",
      "         [-0.1060,  0.0798, -0.2508,  0.0960, -0.2757]],\n",
      "\n",
      "        [[ 0.0434,  0.0873, -0.0879,  0.1100,  0.5907],\n",
      "         [-0.0907,  0.0738,  0.1541,  0.0971,  0.2843],\n",
      "         [-0.2087,  0.2055,  0.2546,  0.2179, -0.1388]]], grad_fn=<CopySlices>)\n",
      "tensor([[-0.1060,  0.0798, -0.2508,  0.0960, -0.2757],\n",
      "        [-0.2087,  0.2055,  0.2546,  0.2179, -0.1388]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.2106,  0.1742, -0.4524,  0.1758, -0.5244],\n",
      "        [-0.5820,  0.2689,  0.3088,  0.3347, -0.1883]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#自己写一个\n",
    "def lstm_forward(inputs, initial_states, w_ih, w_hh, b_ih, b_hh, w_rh=None):\n",
    "    h_0, c_0 = initial_states #初始状态\n",
    "    bs, T, input_size = inputs.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "    prev_h, prev_c = h_0, c_0 # 每次都要传入之前的细胞和隐藏状态\n",
    "    \n",
    "    \n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)  #扩充维度后复制,bs,4*hidden_size, input_size\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)  #bs,4*hidden_size, hidden_size\n",
    "    batch_b_ih = b_ih.unsqueeze(0).tile(bs, 1)  #bs,h_size\n",
    "    batch_b_hh = b_hh.unsqueeze(0).tile(bs, 1)  #bs,h_size\n",
    "    \n",
    "    if w_rh is not None:\n",
    "        p_size = w_rh.shape[0]  \n",
    "        output_size = p_size\n",
    "        batch_w_rh = w_rh.unsqueeze(0).tile(bs, 1, 1)  # bs,p_size,h_size\n",
    "    else:\n",
    "        output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size)\n",
    "        \n",
    "    \n",
    "    \n",
    "    for t in range(T):\n",
    "        x = inputs[:, t, :]  #当前时刻的输入向量 bs*input_size\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(2))  #bs,4*h_size, 1\n",
    "        w_times_x = w_times_x.squeeze(2)  #bs,4*h_size\n",
    "        \n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  #bs,4*h_size,h_size  bs,h_size,1 = bs,4*h_size,1\n",
    "        #如果是projection,bs,4*h_size,p_size  bs,p_size,1 = bs,4*h_size,1\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  #bs,4*h_size\n",
    "        \n",
    "        #分别计算书入门i，遗忘门f，cell门g，输出门o\n",
    "        i_t = torch.sigmoid(w_times_x[:, :1*h_size] + batch_b_ih[:, :1*h_size] + \\\n",
    "                          w_times_h_prev[:, :1*h_size] + batch_b_hh[:, :1*h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + batch_b_ih[:, h_size:2*h_size] + \\\n",
    "                          w_times_h_prev[:, h_size:2*h_size] + batch_b_hh[:, h_size:2*h_size]) \n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + batch_b_ih[:, 2*h_size:3*h_size] + \\\n",
    "                          w_times_h_prev[:, 2*h_size:3*h_size] + batch_b_hh[:, 2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + batch_b_ih[:, 3*h_size:] + \\\n",
    "                          w_times_h_prev[:, 3*h_size:] + batch_b_hh[:, 3*h_size:])\n",
    "        prev_c = f_t*prev_c + i_t*g_t #bs,h_size\n",
    "        prev_h = o_t*torch.tanh(prev_c) #bs,h_size\n",
    "        \n",
    "        if w_rh is not None:\n",
    "            prev_h = torch.bmm(batch_w_rh, prev_h.unsqueeze(-1)).squeeze(2) #bs,p_size,h_size * bs,h_size,1 = bs,p_size,1\n",
    "        output[:, t, :] = prev_h\n",
    "        \n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "forward_output, (forward_hn, forward_cn) = lstm_forward(inputs, (h_0,c_0), lstm_layer.weight_ih_l0, lstm_layer.weight_hh_l0, \\\n",
    "             lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n",
    "print(forward_output)\n",
    "print(forward_hn)\n",
    "print(forward_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4bc5590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0423, -0.0554, -0.0014],\n",
      "         [ 0.0168,  0.0114, -0.0809],\n",
      "         [ 0.0308,  0.0179, -0.0685]],\n",
      "\n",
      "        [[ 0.2101,  0.0282, -0.1115],\n",
      "         [ 0.2058, -0.0541, -0.1189],\n",
      "         [ 0.2325, -0.0897, -0.1474]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.0308,  0.0179, -0.0685],\n",
      "         [ 0.2325, -0.0897, -0.1474]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.0141, -0.2495, -0.5108, -0.1488, -0.0325],\n",
      "         [-0.3937,  0.1852, -0.2235, -0.5974,  0.3615]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_0 = torch.randn(bs, proj_size) #proj是对h压缩输出\n",
    "pro_lstm_layer = nn.LSTM(i_size, h_size, batch_first=True,proj_size=proj_size)\n",
    "output, (hn, cn) = pro_lstm_layer(inputs, (h_0.unsqueeze(0), c_0.unsqueeze(0)))\n",
    "\n",
    "print(output)\n",
    "print(hn)\n",
    "print(cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f405e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0423, -0.0554, -0.0014],\n",
      "         [ 0.0168,  0.0114, -0.0809],\n",
      "         [ 0.0308,  0.0179, -0.0685]],\n",
      "\n",
      "        [[ 0.2101,  0.0282, -0.1115],\n",
      "         [ 0.2058, -0.0541, -0.1189],\n",
      "         [ 0.2325, -0.0897, -0.1474]]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0308,  0.0179, -0.0685],\n",
      "        [ 0.2325, -0.0897, -0.1474]], grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.0141, -0.2495, -0.5108, -0.1488, -0.0325],\n",
      "        [-0.3937,  0.1852, -0.2235, -0.5974,  0.3615]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "forward_output, (forward_hn, forward_cn) = lstm_forward(inputs, (h_0,c_0), pro_lstm_layer.weight_ih_l0, pro_lstm_layer.weight_hh_l0, \\\n",
    "             pro_lstm_layer.bias_ih_l0, pro_lstm_layer.bias_hh_l0, pro_lstm_layer.weight_hr_l0)\n",
    "print(forward_output)\n",
    "print(forward_hn)\n",
    "print(forward_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe5edae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4]) torch.Size([20, 5])\n",
      "torch.Size([20, 4]) torch.Size([20, 3]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(lstm_layer.weight_ih_l0.shape, lstm_layer.weight_hh_l0.shape)\n",
    "print(pro_lstm_layer.weight_ih_l0.shape, pro_lstm_layer.weight_hh_l0.shape,pro_lstm_layer.weight_hr_l0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project是对隐状态作了投影，维度改变，可以使得W_hh矩阵参数减少\n",
    "原本的4*h_size,h_size变成4*h_size,p_size，但也多了一个p_size,h_size的投影矩阵"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
